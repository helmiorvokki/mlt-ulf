{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code divides the data into days with two hours overlap and does the wavelet.\n",
    "You can use netCDF or csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import colors\n",
    "from matplotlib.collections import LineCollection\n",
    "from datetime import datetime, timedelta\n",
    "import pycwt as wavelet\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates and returns the horizontal component of the magnetic field\n",
    "def horizontal(df):\n",
    "    Bn = df['dbn_nez'].to_numpy()\n",
    "    Be = df['dbe_nez'].to_numpy()\n",
    "    Bh = np.sqrt(Bn**2+Be**2)\n",
    "    return Bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that does the wavelet transform for given data\n",
    "def wave(dat):\n",
    "    dt = 60.0 # timestep \n",
    "    mother = wavelet.Morlet(6) \n",
    "    s0 = 2 * dt  # Starting scale, 2 * 60 s = 2 mins\n",
    "    dj = 1 / 12  # Twelve sub-octaves per octaves\n",
    "    J = 9 / dj  # Seven powers of two with dj sub-octaves\n",
    "\n",
    "    # wavelet transform\n",
    "    wave, scales, freqs, coi, fft, fftfreqs = wavelet.cwt(dat, dt, dj, s0, J,\n",
    "                                                    mother)\n",
    "    \n",
    "    # inverse wavelet\n",
    "    iwave = wavelet.icwt(wave, scales, dt, dj, mother)\n",
    "\n",
    "    # ULF power\n",
    "    power = (np.abs(wave)) ** 2\n",
    "    \n",
    "    power /= scales[:, None]\n",
    "\n",
    "    return iwave, power, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the sum of powers in the pc5 range\n",
    "def pc5power(freqs, power):\n",
    "    # choose values in the pc5 frequency range\n",
    "    a = np.where((2e-3 <= freqs) & (freqs <= 7e-3))\n",
    "    low = a[0][0]\n",
    "    up = a[0][-1]\n",
    "    \n",
    "    # sum the powers in the pc5 range\n",
    "    powers = np.sum(power[low:up+1,:],axis=0)\n",
    "\n",
    "    return powers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have netCDF data run the next two cells and skip the CSV data cell.\n",
    "If you have CSV data skip the next two cells and run the csv data cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read netCDF data\n",
    "f = Dataset('filename.netcdf')\n",
    "\n",
    "# get the needed variables\n",
    "Bn = f.variables['dbn_nez'][:]\n",
    "Be = f.variables['dbe_nez'][:]\n",
    "\n",
    "yr = f.variables['time_yr'][:] #year\n",
    "mo = f.variables['time_mo'][:] #months\n",
    "day = f.variables['time_dy'][:] #days\n",
    "hr = f.variables['time_hr'][:] #hours\n",
    "mt = f.variables['time_mt'][:] #minutes\n",
    "mlat = f.variables['mlat'][:] #magnetic latitudes\n",
    "glon = f.variables['glon'][:] #geographic longitude\n",
    "glat = f.variables['glat'][:] #geographic latitude\n",
    "mlt = f.variables['mlt'][:]\n",
    "stations = f.variables['id'][0][:]\n",
    "\n",
    "lon = glon[0]\n",
    "lat = glat[0]\n",
    "\n",
    "# make dates into datetime object\n",
    "date_UTC = [datetime(year=yr[i], month=mo[i], day=day[i],hour=hr[i],minute=mt[i]) for i in range(len(mo))]\n",
    "\n",
    "# take stations between 60 and 70 magnetic latitudes\n",
    "s6070 = np.where((mlat[0] >= 60) & (mlat[0] <70))\n",
    "\n",
    "lons = lon[s6070]\n",
    "lats = lat[s6070]\n",
    "\n",
    "df_CDF = []\n",
    "\n",
    "for i in s6070[0]:\n",
    "    # make a dataframe of the relevant data\n",
    "    data_df = {'dbn_nez': Bn[:,i], 'dbe_nez':Be[:,i], 'MLT':mlt[:,i]}\n",
    "    # append the dataframe to a list\n",
    "    df_CDF.append(pd.DataFrame(data=data_df, index=date_UTC).rename_axis(index='Date_UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ULF power from netCDF data\n",
    "\n",
    "# timestamps to divide data into days\n",
    "time = df_CDF[0].index\n",
    "start = time[0]\n",
    "timerange = len(time)/(60*24)\n",
    "end = start + timedelta(days=timerange)\n",
    "indices = [start + timedelta(days=i) for i in range(int(timerange))]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for df in df_CDF:\n",
    "    for date in indices:\n",
    "        if date == start: # adds 2 hours of data to the end\n",
    "            s = date\n",
    "            e = date + timedelta(days=1, hours=2)\n",
    "\n",
    "        elif date == end -timedelta(days=1): # adds two hours of data to the beginning\n",
    "            s = date - timedelta(hours=2)\n",
    "            e = end\n",
    "\n",
    "        else: # adds two hours of data to both sides\n",
    "            s = date -timedelta(hours=2)\n",
    "            e = date + timedelta(days=1, hours=2)\n",
    "\n",
    "        # take the day in question (plus the overlaps)\n",
    "        df1 = df[(df.index >= s) & (df.index < e)]\n",
    "        part = df1.index # times\n",
    "        \n",
    "        # get the horizontal component and make a series\n",
    "        B = horizontal(df1)\n",
    "        ser = pd.Series(data=B, index=part)\n",
    "\n",
    "        # maximum count of consecutive nans\n",
    "        nans = ser.isnull().astype(int).groupby(ser.notnull().astype(int).cumsum()).sum().max()\n",
    "\n",
    "        # if there is a datagap larger than 40 minutes or the data starts with zero, do not include \n",
    "        if nans > 40 or ser.isna()[0] == True:\n",
    "            continue\n",
    "\n",
    "        # interpolate for possible nans\n",
    "        ser = ser.interpolate()\n",
    "        ser.resample('1s').fillna('bfill')\n",
    "        B = ser.to_numpy()\n",
    "        \n",
    "        # get the wavepower\n",
    "        iwave, power, freqs = wave(B)\n",
    "\n",
    "        # pc5 power\n",
    "        pc5 = pc5power(freqs, power)\n",
    "        dat = {'pc5' : pc5, 'MLT' : df1['MLT'].to_numpy()}\n",
    "        df2 = pd.DataFrame(data=dat, index=part)\n",
    "\n",
    "        # remove overlaps and save to a list\n",
    "        dfs.append(df2[(df2.index>= date) & (df2.index < date+ timedelta(days=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using csv data\n",
    "If you are using netCDF data skip this next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data\n",
    "df = pd.read_csv('filename.csv')\n",
    "\n",
    "# convert dates into datetime \n",
    "format='%Y-%m-%dT%H:%M:%S'\n",
    "df['Date_UTC'] = pd.to_datetime(df['Date_UTC'], format=format)\n",
    "\n",
    "# get the time of the whole data\n",
    "times = df.groupby('Date_UTC').mean()\n",
    "\n",
    "# get names of the stations\n",
    "names = df.groupby('IAGA').count().index.to_numpy()\n",
    "\n",
    "# timestamps to divide data into days\n",
    "start = times.index[0]\n",
    "timerange = len(times)/(60*24)\n",
    "end = start + timedelta(days=timerange)\n",
    "indices = [start + timedelta(days=i) for i in range(int(timerange))]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# go over dates\n",
    "for date in indices:\n",
    "    if date == start: # adds 2 hours of data to the end\n",
    "        s = date\n",
    "        e = date + timedelta(days=1, hours=2)\n",
    "\n",
    "    elif date == end -timedelta(days=1): # adds two hours of data to the beginning\n",
    "        s = date - timedelta(hours=2)\n",
    "        e = end\n",
    "\n",
    "    else: # adds two hours of data to both sides\n",
    "        s = date -timedelta(hours=2)\n",
    "        e = date + timedelta(days=1, hours=2)\n",
    "\n",
    "    # go through all stations \n",
    "    for name in names:\n",
    "        # take the station and day in question\n",
    "        a = df.groupby('IAGA').get_group(name)\n",
    "        df1 = a[(a['Date_UTC'] >= s) & (a['Date_UTC'] < e)]\n",
    "        time = df1['Date_UTC']\n",
    "\n",
    "        B = horizontal(df1)\n",
    "        ser = pd.Series(data=B, index=df1['Date_UTC'])\n",
    "\n",
    "        # maximum count of consecutive nans\n",
    "        nans = ser.isnull().astype(int).groupby(ser.notnull().astype(int).cumsum()).sum().max()\n",
    "        \n",
    "        # if there is a datagap larger than 40 minutes or the data starts with zero, do not include \n",
    "        if nans > 40 or ser.isna()[0] == True:\n",
    "            continue\n",
    "\n",
    "        # interpolate possible nan values\n",
    "        ser = ser.interpolate()\n",
    "        ser.resample('1s').fillna('bfill')\n",
    "        B = ser.to_numpy()\n",
    "        \n",
    "        # get the wavepower\n",
    "        iwave, power, freqs = wave(B, 60.0)\n",
    "\n",
    "        # pc5 power\n",
    "        pc5 = pc5power(freqs, power)\n",
    "        dat = {'pc5' : pc5, 'MLT' : df1['MLT'].to_numpy()}\n",
    "        df2 = pd.DataFrame(data=dat, index=time)\n",
    "\n",
    "        # remove overlaps and save to a list\n",
    "        dfs.append(df2[(df2.index >= date) & (df2.index < date + timedelta(days=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cells are the same for both kinds of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes together and calculate the mean pc5 power\n",
    "con = pd.concat(dfs)\n",
    "global_ULF = con.groupby('Date_UTC')['pc5'].mean()\n",
    "time = global_ULF.index\n",
    "\n",
    "# get hourly mean of pc5 power\n",
    "glbl_ULF = pd.Series(global_ULF.values, name='ULF power', index=time)\n",
    "hmean = glbl_ULF.resample('1H').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data into different MLT sectors\n",
    "\n",
    "idx = pd.date_range(min(time), max(time), freq='T')\n",
    "\n",
    "# ULF power in the night sector\n",
    "MLT213 = con[((con['MLT']>= 21) & (con['MLT'] <= 24)) | ((con['MLT']>= 0) & (con['MLT'] < 3))]\n",
    "night = MLT213.groupby('Date_UTC')['pc5'].mean()\n",
    "nightstat = MLT213.groupby('Date_UTC').size()\n",
    "\n",
    "# if dates are missing, add zeros to them\n",
    "if len(night) != len(time):\n",
    "    night = night.reindex(idx, fill_value=0)\n",
    "    nightstat = nightstat.reindex(idx, fill_value=0)\n",
    "\n",
    "# ULF power in the dawn sector\n",
    "MLT39 = con[(con['MLT'] >= 3) & (con['MLT'] < 9)]\n",
    "dawn = MLT39.groupby('Date_UTC')['pc5'].mean()\n",
    "dawnstat = MLT39.groupby('Date_UTC').size()\n",
    "\n",
    "# if dates are missing, add zeros to them\n",
    "if len(dawn) != len(time):\n",
    "    dawn = dawn.reindex(idx, fill_value=0)\n",
    "    dawnstat = dawnstat.reindex(idx, fill_value=0)\n",
    "\n",
    "# ULF power in the day sector\n",
    "MLT915 = con[(con['MLT'] >= 9) & (con['MLT'] < 15)]\n",
    "day = MLT915.groupby('Date_UTC')['pc5'].mean()\n",
    "daystat = MLT915.groupby('Date_UTC').size()\n",
    "\n",
    "# if dates are missing, add zeros to them\n",
    "if len(day) != len(time):\n",
    "    day = day.reindex(idx, fill_value=0)\n",
    "    daystat = daystat.reindex(idx, fill_value=0)\n",
    "\n",
    "# ULF power in the dusk sector\n",
    "MLT1521 = con[(con['MLT'] >= 15) & (con['MLT'] < 21)]\n",
    "dusk = MLT1521.groupby('Date_UTC')['pc5'].mean()\n",
    "duskstat = MLT1521.groupby('Date_UTC').size()\n",
    "\n",
    "# if dates are missing, add zeros to them\n",
    "if len(dusk) != len(time):\n",
    "    dusk = dusk.reindex(idx, fill_value=0)\n",
    "    duskstat = duskstat.reindex(idx, fill_value=0)\n",
    "\n",
    "# add together\n",
    "datas = {'D':night.index.day, 'H':night.index.hour, 'MIN' : night.index.minute, 'Tdawn':dawn.to_numpy(), 'Tday':day.to_numpy(), 'Tdusk':dusk.to_numpy(), 'Tnight':night.to_numpy(),'Tmean':global_ULF.to_numpy()}\n",
    "d = pd.DataFrame(data=datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to daily files\n",
    "for i in indices:\n",
    "    dayw = d[(d['D']==i.day)]\n",
    "    path = '/home/live/ULF_index/'+i.strftime('%Y')+'/'+i.strftime('%m')+'/'+i.strftime('%d')+'.asc'\n",
    "    with open(path, \"w\") as f:\n",
    "        df_string = dayw.to_string(index=False)\n",
    "        f.write(df_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cells are for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ULF INDICES\n",
    "import read_virbo_ULF as VULF\n",
    "\n",
    "#start\n",
    "year_s = 2001\n",
    "month_s = 1\n",
    "day_s = 1\n",
    "start_time = str(year_s)+'-'+str(month_s)+'-'+str(day_s)\n",
    "\n",
    "#end\n",
    "year_e = 2001\n",
    "month_e = 12\n",
    "day_e = 31\n",
    "end_time = str(year_e)+'-'+str(month_e)+'-'+str(day_e)\n",
    "\n",
    "startTime = datetime(year_s, month_s, day_s)\n",
    "stopTime = datetime(year_e, month_e, day_e)\n",
    "\n",
    "timeTgr, Tgr, Tgeo , Tgr_detrend, Tgeo_detrend= VULF.read_ULF_asc_data(startTime, stopTime)\n",
    "\n",
    "dates = []\n",
    "for i in timeTgr:\n",
    "    dates.append(datetime.fromtimestamp(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a multicolor line\n",
    "def multicolor(y, s, ax):\n",
    "    # plot grey lines\n",
    "    ax.axhline(1e-3, c='grey', alpha=0.4, ls='--')\n",
    "    ax.axhline(1e-1, c='grey', alpha=0.4, ls='--')\n",
    "    ax.axhline(1e1, c='grey', alpha=0.4, ls='--')\n",
    "    ax.axhline(1e3, c='grey', alpha=0.4, ls='--')\n",
    "\n",
    "    xval = mdates.date2num(global_ULF.index)\n",
    "    points = np.array([xval, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"\", [\"pink\",\"hotpink\",\"deeppink\",\"mediumvioletred\"])\n",
    "    s = np.ma.masked_where(s<1,s)\n",
    "    cmap.set_bad(color='white')\n",
    "    lc = LineCollection(segments, cmap=cmap)\n",
    "    lc.set_array(s)\n",
    "    lc.set_linewidth(3)\n",
    "\n",
    "    line = ax.add_collection(lc)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ULF data\n",
    "fontsize=18\n",
    "\n",
    "fig, ax = plt.subplots(7, 1, figsize=(21,25), sharex='all')\n",
    "fig.set(facecolor='white')\n",
    "fig.suptitle('Magnetic latitude: 60â€“70, horizontal B', fontsize=20)\n",
    "\n",
    "l = multicolor(night, nightstat, ax[0])\n",
    "ax[0].set_ylabel('Night', fontsize=fontsize)\n",
    "ax[0].set_yscale('log')\n",
    "\n",
    "multicolor(dawn, dawnstat, ax[1])\n",
    "ax[1].set_ylabel('Dawn', fontsize=fontsize)\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "multicolor(day, daystat, ax[2])\n",
    "ax[2].set_ylabel('Day', fontsize=fontsize)\n",
    "ax[2].set_yscale('log')\n",
    "\n",
    "multicolor(dusk, duskstat, ax[3])\n",
    "ax[3].set_ylabel('Dusk', fontsize=fontsize)\n",
    "ax[3].set_yscale('log')\n",
    "\n",
    "ax[4].plot(time, global_ULF, color='darkviolet')\n",
    "ax[4].set_ylabel('mean pc5 (min)', fontsize=fontsize)\n",
    "\n",
    "ax[5].plot(hmean.index, hmean.values, color='darkviolet')\n",
    "ax[5].set_ylabel('mean pc5 (h)', fontsize=fontsize)\n",
    "\n",
    "ax[6].plot(dates, Tgr, color='darkviolet')\n",
    "ax[6].set_ylabel(r'$\\rm{T_{gr}}$', fontsize=fontsize)\n",
    "ax[6].set_xlim([time.min(), time.max()])\n",
    "#ax[6].set_yscale('log')\n",
    "ax[6].tick_params(axis='x', labelrotation = 40)\n",
    "ax[6].xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M:%S'))\n",
    "\n",
    "plt.rc('xtick', labelsize=16)    \n",
    "plt.rc('ytick', labelsize=15) \n",
    "\n",
    "cax = plt.axes([0.92, 0.57, 0.015, 0.3])\n",
    "fig.colorbar(l,cax=cax)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.0)\n",
    "fig.align_ylabels()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
